"""
Class object to measure feature important using majority voting across n-estimators.

"""
import os
import logging
import git
import pandas as pd
import numpy as np
from typing import Dict, Union, List, Tuple
from collections import Counter
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin

# Feature Importance
import shap

# Data
from sklearn.datasets import make_classification, make_regression

# Processing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from category_encoders.count import CountEncoder

# Use the git library to find the root directory of the project.  return a string value.
DIR_ROOT = git.Repo(".", search_parent_directories=True).working_tree_dir
DIR_DATA = os.path.join(DIR_ROOT, "data")
DIR_DATA_EXAMPLES = os.path.join(DIR_DATA, "examples")

# Settings
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


class DataGenerator:
    """
    This DataGenerator class provides a framework for handling data preparation tasks in a machine learning pipeline.
    It allows users to specify various options like synthetic data generation, feature selection, and test-train split
    ratio, making it adaptable to different scenarios.

    Attributes:
    ===========
    The class has several attributes that store information about the data and its manipulation:

    data (pd.DataFrame): Holds the main data as a Pandas DataFrame.
    objective (str): Specifies the objective of the machine learning task, either "classification" or "regression".
    target_column (str): Name of the target column in the data.
    feature_set (list): List of all feature names used in the analysis.
    numeric_features (list): Subset of features classified as numerical.
    categorical_features (list): Subset of features classified as categorical.
    generate_synthetic_data (bool): Flag indicating whether to generate synthetic data if none is provided.
    test_size (float): Proportion of data to be used for testing.
    synthetic_samples (int): Number of samples to generate in synthetic data creation.
    X (pd.DataFrame): DataFrame holding all features (excluding target).
    y (pd.DataFrame): DataFrame holding the target variable.
    X_train (pd.DataFrame): Training set features.
    X_test (pd.DataFrame): Testing set features.
    y_train (pd.DataFrame): Training set target variable.
    y_test (pd.DataFrame): Testing set target variable.
    data_generator (dict): Dictionary mapping objective type (classification/regression) to the corresponding data generating function.

    Methods:
    ========
    _generate_synthetic_data (self):
    Generates synthetic data if the generate_synthetic_data flag is True.
    Defines feature sets, target column, and populates the data attribute using the appropriate data generation function from data_generator.
    _split_features (self):
    Splits the data into features (X) and target variable (y) DataFrames.
    Defines the feature set based on the intersection of provided features and existing columns in the data.
    Raises an assertion error if features are not found in the data.
    _generate_train_test_split (self):
    Splits the features and target variable into training and testing sets using train_test_split from scikit-learn.
    Stratifies the split based on the target variable if the objective is classification.
    Reports the shapes of the resulting training and testing sets.
    build (self):
    Calls the internal methods (_generate_synthetic_data, _split_features, and _generate_train_test_split) sequentially to perform data preparation.
    Returns the self object, allowing for method chaining.
    """

    def __init__(
        self,
        objective: str,
        data: pd.DataFrame,
        target_column: str,
        feature_set: list,
        numeric_features: list,
        categorical_features: list,
        test_size: float = 0.33,
        generate_synthetic_data: bool = False,
    ):
        self.data = data
        self.objective = objective
        self.target_column = target_column
        self.feature_set = feature_set
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        self.generate_synthetic_data = generate_synthetic_data
        self.test_size = test_size
        self.synthetic_samples: int = 1_000
        # Generated by this class object
        self.X = pd.DataFrame({})
        self.y = pd.DataFrame({})
        self.X_train = pd.DataFrame({})
        self.X_test = pd.DataFrame({})
        self.y_train = pd.DataFrame({})
        self.y_test = pd.DataFrame({})
        self.data_generator: dict = {
            "classification": make_classification,
            "regression": make_regression,
        }

        assert (
            self.objective in self.data_generator.keys()
        ), "Objective must be either classification or regression"
        logger.info(f"Class object {self.__class__.__name__} instantiated successfully")

    def _generate_synthetic_data(self):
        """
        Generate synthetic data if no data is provided.
        """
        if self.generate_synthetic_data:
            logger.info("Generating synthetic data.")
            self.feature_set = [f"feature_{i}" for i in range(20)]
            self.categorical_features = self.feature_set[:10]
            self.numeric_features = self.feature_set[10:]
            self.target_column = "TARGET"
            self.data = pd.DataFrame(
                self.data_generator[self.objective](
                    n_samples=self.synthetic_samples,
                    n_features=len(self.feature_set),
                    n_informative=int(0.5 * len(self.feature_set)),
                    n_classes=2,
                    random_state=42,
                )[0],
                columns=self.feature_set,
            )
            self.data[self.target_column] = self.data_generator[self.objective](
                n_samples=self.synthetic_samples, random_state=42
            )[1]
            self.feature_set = [
                col for col in self.data.columns if col != self.target_column
            ]
        return self

    def _split_features(self):
        """
        Split the features into X & Y.  Define feature set.
        :return:
        """
        # If X & y are empty np arrays.
        logger.info("Splitting the data into X & y.")
        self.y = self.data[self.target_column]
        self.X = self.data.drop(self.target_column, axis=1)

        conditions = [
            [f for f in self.numeric_features if f in self.feature_set],
            [f for f in self.categorical_features if f in self.feature_set],
        ]
        msg = "Features not found in feature set. \n Feature Set {}\n Numeric {}, \n Categorical {}".format(
            self.feature_set, self.numeric_features, self.categorical_features
        )
        assert all(conditions), msg

        return self

    def _generate_train_test_split(self):
        logger.info("Generating the train test split.")
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X,
            self.y,
            test_size=self.test_size,
            stratify=self.y if self.objective == "classification" else None,
        )
        logger.info(
            "\t X_train shape: {}, y_train shape: {}, X_test shape: {}, y_test shape: {}".format(
                self.X_train.shape,
                self.y_train.shape,
                self.X_test.shape,
                self.y_test.shape,
            )
        )
        return self

    def build(self):
        self._generate_synthetic_data()
        self._split_features()
        self._generate_train_test_split()
        return self


class DataTransformerPipelineBuilder:
    """
    This DataTransformerPipelineBuilder class helps define pipelines for transforming both numeric and categorical
    features in a dataset. It provides a structured approach for handling missing values, scaling numeric features,
    and encoding categorical features, making it easier to prepare data for machine learning models.

    Attributes:
    ==========
    The class has several attributes that store information about the feature sets and the constructed transformers:

    feature_set (list): List containing all feature names in the data.
    numeric_features (list): Subset of features classified as numerical.
    categorical_features (list): Subset of features classified as categorical.
    numeric_pipeline (Pipeline): Pipeline object containing transformation steps for numeric features.
    categorical_pipeline (Pipeline): Pipeline object containing transformation steps for categorical features.
    column_transformer (ColumnTransformer): Object combining both pipelines for unified feature transformation.

    Methods:
    ========
    The class includes several methods that perform specific tasks related to building the transformer pipelines:

    _build_numeric_column_pipeline (self):
        Defines a pipeline for numeric feature transformations:
        SimpleImputer with "median" strategy to handle missing values.
        StandardScaler for standardization.
        Assigns the constructed pipeline to the numeric_pipeline attribute.
        Returns self for method chaining.
    _build_categorical_pipeline (self):
        Defines a pipeline for categorical feature transformations:
        CountEncoder with "value" strategies for handling unknown and missing categories.
        SimpleImputer with "most_frequent" strategy to handle missing values.
        Assigns the constructed pipeline to the categorical_pipeline attribute.
        Returns self for method chaining.
    _build_column_transformer (self):
        Creates a ColumnTransformer object that combines the individual pipelines for numeric and categorical features:
        "num" transformer applies the numeric_pipeline to corresponding numeric feature indices.
        "cat" transformer applies the categorical_pipeline to corresponding categorical feature indices.
        Assigns the constructed ColumnTransformer to the column_transformer attribute.
        Returns self for method chaining.
    build (self):
        Calls the internal methods (_build_numeric_column_pipeline, _build_categorical_pipeline, and _build_column_transformer) sequentially to build the transformer pipelines.
        Returns the self object, allowing for method chaining.

    """

    def __init__(
        self, feature_set, numeric_features: List[str], categorical_features: List[str]
    ):
        self.feature_set = feature_set
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        # Created by this class object
        self.numeric_pipeline = Pipeline
        self.categorical_pipeline = Pipeline
        self.column_transformer = ColumnTransformer(transformers=[])
        msg = (
            "Feature set must be equal to the sum of numeric and categorical features."
            + "Feature Sets Recieved Numeric: {} Categorical: {} All {}".format(
                numeric_features, categorical_features, feature_set
            )
        )
        assert not set(numeric_features + categorical_features).difference(
            feature_set
        ), msg

    def _build_numeric_column_pipeline(self):
        logger.info("Building the numeric transformer.")
        self.numeric_pipeline = Pipeline(
            steps=[
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
            ]
        )
        return self

    def _build_categorical_pipeline(self):
        logger.info("Building the categorical transformer.")
        self.categorical_pipeline = Pipeline(
            steps=[
                (
                    "encoder",
                    CountEncoder(handle_unknown="value", handle_missing="value"),
                ),
                ("imputer", SimpleImputer(strategy="most_frequent")),
            ]
        )
        return self

    def _build_column_transformer(self):
        logger.info("Building the column transformer.")
        self.column_transformer = ColumnTransformer(
            transformers=[
                (
                    "num",
                    self.numeric_pipeline,
                    [self.feature_set.index(f) for f in self.numeric_features],
                ),
                (
                    "cat",
                    self.categorical_pipeline,
                    [self.feature_set.index(f) for f in self.categorical_features],
                ),
            ]
        )
        return self

    def build(self):
        self._build_numeric_column_pipeline()
        self._build_categorical_pipeline()
        self._build_column_transformer()
        return self


class EstimatorPipelineBuilder:
    def __init__(
        self,
        estimators: List[
            Tuple[str, Union[BaseEstimator, ClassifierMixin, RegressorMixin]]
        ],
        column_transformer: ColumnTransformer,
    ):
        self.estimators = estimators
        self.column_transformer = column_transformer
        # Created by this class
        self.estimator_pipelines = {}

    def _build_estimator_pipelines(self):
        assert self.estimators, "No estimators found."
        logger.info("Building estimator pipelines.")
        for name, estimator in self.estimators:
            conditions = [
                hasattr(estimator, "predict"),
                hasattr(estimator, "fit"),
            ]
            if not all(conditions):
                logger.warning(
                    f"\t Unable to Add Estimator {name} as it lacks the required methods."
                )
            else:
                logger.info(f"\t\t Adding Estimator {name}")
                self.estimator_pipelines[name] = Pipeline(
                    steps=[
                        ("preprocessor", self.column_transformer),
                        ("estimator", estimator),
                    ]
                )
        return self

    def build(self):
        self._build_estimator_pipelines()
        return self


class FeatureImportanceGenerator:
    """
    Calculates and analyzes feature importance for different machine learning models.

    This class facilitates the generation and analysis of feature importance for various
    machine learning models provided in estimator pipelines. It can also plot feature importance
    and identify significant features based on individual model importance and a voting scheme.

    Attributes:
    ===========
    The class has several attributes that store information about the pipelines, data, and generated importance scores:

    estimator_pipelines (dict): Dictionary containing scikit-learn pipelines with trained models, each identified by a name.
    X_train (pd.DataFrame): Training features.
    X_test (pd.DataFrame): Testing features.
    y_train: Training target variable.
    feature_set (list): List containing all feature names.
    num_labels: Number of unique labels in the target variable (relevant for classification tasks).
    plot_importance (bool): Flag indicating whether to plot feature importance using SHAP.
    feature_importance_df (pd.DataFrame): DataFrame holding feature importance scores for each model.
    estimator_feature_importance (dict): Dictionary storing individual feature importance dataframes for each model.
    data_transformed (bool): Flag indicating whether data transformation has been performed.

    Methods:
    ========
    The class includes several methods that perform specific tasks related to feature importance calculations and analysis:

    _transform_data (self, name, pipeline):
        Transforms the data using the preprocessor step in a specific pipeline (specified by name).
        Sets the data_transformed flag to True.
    _generate_feature_importance (self, name, pipeline):
        Extracts the final estimator from the pipeline.
        Fits the estimator on the training data.
        Optionally plots feature importance using SHAP (if plot_importance is True).
        Creates an explainer object using SHAP and calculates feature importance scores (mean absolute values) for the test data.
        Returns a DataFrame containing feature names and calculated importance scores.
    _join_feature_importance (self):
        Creates a list of individual importance DataFrames from the estimator_feature_importance dictionary.
        Concatenates the DataFrames into a single feature_importance_df and renames the columns to match model names.
    _get_importance_features (self):
        Iterates through each model's importance column in the feature_importance_df.
        Calculates the median importance value for each model.
        Creates a new column with a flag indicating whether a feature is significant based on exceeding the model's median importance.
    _get_total_feature_votes (self):
        Identifies columns in the DataFrame that contain significance flags.
        Calculates the sum of significance flags across these columns (total votes for each feature).
        Adds a new column "TOTAL_VOTES" to the DataFrame containing the calculated total votes.
    _get_majority_vote (self):
        Calculates a new column "IS_MAJORITY" indicating whether a feature received a majority vote (N-1 models considering it significant).
    build (self):
        Ensures at least one estimator pipeline is provided using an assertion.
        Iterates through each model pipeline:
        Transforms the data using the pipeline's preprocessor if not already done.
        Calls _generate_feature_importance to calculate importance scores for the current model.
        Joins the individual importance DataFrames into a single one.
        Analyzes feature importance and adds additional columns with significance flags and voting results.
        Returns the self object for method chaining.
    """

    def __init__(
        self,
        estimator_pipelines,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train,
        feature_set: list,
        plot_importance: bool = False,
    ):
        self.estimator_pipelines = estimator_pipelines
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.feature_set = feature_set
        self.num_labels = len(Counter(self.y_train))
        self.plot_importance = plot_importance
        self.feature_importance_df = pd.DataFrame({})
        self.estimator_feature_importance = {}
        self.data_transformed = False

    def _transform_data(self, name: str, pipeline: Pipeline):
        """
        Transform the data using the column transformer.
        """
        if not self.data_transformed:
            logger.info("Transforming the data.")
            self.X_train = (
                self.estimator_pipelines["RandomForestClassifier"]
                .named_steps["preprocessor"]
                .fit_transform(self.X_train)
            )
            self.X_test = (
                self.estimator_pipelines["RandomForestClassifier"]
                .named_steps["preprocessor"]
                .transform(self.X_test)
            )
        self.data_transformed = True
        return self

    def _generate_feature_importance(self, name: str, pipeline: Pipeline):
        """ """
        logger.info(f"\t Generating feature importance for {name}")
        # Access the final estimator from the pipeline
        estimator = pipeline.named_steps["estimator"]

        # Fit Estimator
        estimator.fit(self.X_train, self.y_train)

        # Plot
        if self.plot_importance:
            explainer = shap.Explainer(estimator)
            shap_values = explainer(self.X_train)
            for i in range(self.num_labels):
                logger.info(f"Feature Importance Class => {i}")
                shap.plots.waterfall(shap_values[0, :, i])

        # Create an Explainer for the estimator and obtain shap values
        explainer = shap.Explainer(estimator, self.X_train)
        shap_values = explainer(self.X_test).values
        # Calculate average SHAP values (absolute or mean, choose based on your preference)
        shap_abs = np.abs(shap_values)  # Calculate absolute values
        # For classification take average of the shap values
        mean_abs_importance = shap_abs.mean(axis=0)
        # Check if Classification and the shape of the mean_abs_importance is 2D
        if len(mean_abs_importance.shape) != 1:
            # Check if second dimension is > 2 meaning we have multiple classes
            if mean_abs_importance.shape[1] >= 2:
                # If two dimensions shap is returning values per class.  we take mean as both values are the same.
                mean_abs_importance = mean_abs_importance.mean(axis=1)

        # Create the DataFrame
        feature_imp_df = pd.DataFrame(
            {
                f"{name}_IMPORTANCE": mean_abs_importance,
            },
            index=self.feature_set,
        )
        return feature_imp_df

    def _join_feature_importance(self):
        """ """
        logger.info("Joining feature importance data.")
        # Create a list of dataframes from the feature importance dictionary
        dfs = [df for df in self.estimator_feature_importance.values()]
        # Join the dataframes
        self.feature_importance_df = pd.concat(dfs, axis=1)
        # Rename the columns
        self.feature_importance_df.columns = [
            f"{k}" for k in self.estimator_feature_importance.keys()
        ]
        return self

    def _get_importance_features(self):
        for model_name in self.feature_importance_df.columns:
            # Calculate Median Importance Value
            model_median_val = self.feature_importance_df[model_name].median()
            logger.info(f"Median value for {model_name} is {model_median_val}")

            # Create Significance Flag
            self.feature_importance_df[f"{model_name}_IS_SIGNIFICANT"] = list(
                map(
                    lambda x: 1 if x >= model_median_val else 0,
                    self.feature_importance_df[model_name],
                )
            )
        return self

    def _get_total_feature_votes(self):
        """
        Calculate total significance votes.
        """
        logger.info("Calculating total votes.")
        significance_columns = [
            c for c in self.feature_importance_df.columns if "IS_SIGNIFICANT" in c
        ]
        self.feature_importance_df["TOTAL_VOTES"] = (
            self.feature_importance_df[significance_columns].sum(axis=1).values
        )
        return self

    def _get_majority_vote(self):
        """
        Function to create a majority vote column.
        If N-1 models agree a feature is important then 1 else 0.
        """
        logger.info("Calculating majority vote.")
        self.feature_importance_df["IS_MAJORITY"] = list(
            map(
                lambda x: 1
                if x >= (len(self.estimator_feature_importance.keys()) - 1)
                else 0,
                self.feature_importance_df["TOTAL_VOTES"],
            )
        )
        return self

    def build(self):
        assert self.estimator_pipelines, "No estimators found."
        logger.info("Building Estimators.")

        for name, pipeline in self.estimator_pipelines.items():
            logger.info(f"\t Fitting Estimator {name}.")
            self._transform_data(name, pipeline)
            self.estimator_feature_importance[name] = self._generate_feature_importance(
                name, pipeline
            )
        self._join_feature_importance()
        self._get_importance_features()
        self._get_total_feature_votes()
        self._get_majority_vote()
        return self


class FeatureImportanceBaseClass:
    """
    Base class for building and analyzing feature importance for multiple machine learning models.

    This class serves as a foundation for building model pipelines and calculating feature importance for multiple
    machine learning models. It facilitates data preparation, feature transformation, model pipeline construction,
    and feature importance analysis with voting and significance flags.

    Attributes:
    ===========
    The class has several attributes that store information about the configuration, data, and generated objects:

    objective (str): Specifies the objective of the machine learning task ("classification" or "regression").
    data (pd.DataFrame): Main data as a Pandas DataFrame.
    target_column (str): Name of the target column in the data.
    data_generator (DataGenerator): Instance of DataGenerator class responsible for data handling.
    data_transformer_pipeline_builder (DataTransformerPipelineBuilder): Instance of DataTransformerPipelineBuilder class responsible for building feature transformation pipelines.
    estimator_pipeline_builder (EstimatorPipelineBuilder): Instance of EstimatorPipelineBuilder class responsible for constructing pipelines with trained models.
    feature_importance_generator (FeatureImportanceGenerator): Instance of FeatureImportanceGenerator class responsible for feature importance calculations and analysis.
    plot_importance (bool): Flag indicating whether to plot feature importance using SHAP.
    test_size (float): Proportion of data to be used for testing.
    feature_set (list): List containing all feature names.
    numeric_features (list): Subset of features classified as numerical.
    categorical_features (list): Subset of features classified as categorical.
    generate_synthetic_data (bool): Flag indicating whether to generate synthetic data if none is provided.
    estimators (List[tuple[str, Union[BaseEstimator, ClassifierMixin, RegressorMixin]]): List of tuples containing model names and corresponding scikit-learn estimator objects.

    Methods:
    ========
    The class includes two primary methods:

    fit (self):
        Creates instances of dependent classes:
        DataGenerator: Handles data preparation and splitting.
        DataTransformerPipelineBuilder: Builds pipelines for feature transformation.
        EstimatorPipelineBuilder: Constructs pipelines with trained models.
        Calls the build method on each created instance to perform their respective tasks.
        Returns the self object for method chaining.
    fit_transform (self):
        Calls the fit method to ensure model pipelines are built.
        Creates an instance of FeatureImportanceGenerator.
        Calls the build method on the FeatureImportanceGenerator to calculate and analyze feature importance.
        Returns the generated DataFrame containing feature importance information.
        Overall, this FeatureImportanceBaseClass serves as a central point for managing
    """

    def __init__(
        self,
        objective: str,
        data: pd.DataFrame,
        target_column: str,
        feature_set: list,
        numeric_features: list,
        categorical_features: list,
        generate_synthetic_data: bool = False,
        plot_importance: bool = False,
        test_size: float = 0.33,
        estimators: List[
            tuple[str, Union[BaseEstimator, ClassifierMixin, RegressorMixin]]
        ] = (),
    ) -> None:
        # Methods
        self.data_generator = None
        self.data_transformer_pipeline_builder = None
        self.estimator_pipeline_builder = None
        self.feature_importance_generator = None
        # Attributes
        self.objective = objective
        self.data = data
        self.target_column = target_column
        self.plot_importance = plot_importance
        self.test_size = test_size
        self.feature_set = feature_set
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        self.generate_synthetic_data = generate_synthetic_data
        self.estimators = estimators
        msg = "number of estimators must be between 2 and 10"
        assert all([len(self.estimators) >= 2, len(self.estimators) <= 10]), msg

    def fit(self):
        """
        Fit the model.
        """
        # Composite Classes
        self.data_generator = DataGenerator(
            data=self.data,
            target_column=self.target_column,
            feature_set=self.feature_set,
            numeric_features=self.numeric_features,
            categorical_features=self.categorical_features,
            generate_synthetic_data=self.generate_synthetic_data,
            objective=self.objective,
            test_size=self.test_size,
        ).build()
        self.data_transformer_pipeline_builder = DataTransformerPipelineBuilder(
            feature_set=self.data_generator.feature_set,
            numeric_features=self.data_generator.numeric_features,
            categorical_features=self.data_generator.categorical_features,
        ).build()
        self.estimator_pipeline_builder = EstimatorPipelineBuilder(
            estimators=self.estimators,
            column_transformer=self.data_transformer_pipeline_builder.column_transformer,
        ).build()

        return self

    def fit_transform(self):
        self.fit()
        self.feature_importance_generator = FeatureImportanceGenerator(
            estimator_pipelines=self.estimator_pipeline_builder.estimator_pipelines,
            X_train=self.data_generator.X_train,
            X_test=self.data_generator.X_test,
            y_train=self.data_generator.y_train,
            feature_set=self.data_generator.feature_set,
            plot_importance=self.plot_importance,
        ).build()
        return self.feature_importance_generator.feature_importance_df


class FeatureImportanceClassification(FeatureImportanceBaseClass):
    def __init__(
        self,
        data: pd.DataFrame,
        target_column: str,
        feature_set: list,
        numeric_features: list,
        categorical_features: list,
        generate_synthetic_data: bool = False,
        plot_importance: bool = False,
        test_size: float = 0.33,
        estimators: List[
            tuple[str, Union[BaseEstimator, ClassifierMixin, RegressorMixin]]
        ] = (),
    ) -> None:
        super().__init__(
            objective="classification",
            estimators=estimators,
            data=data,
            target_column=target_column,
            plot_importance=plot_importance,
            test_size=test_size,
            feature_set=feature_set,
            numeric_features=numeric_features,
            categorical_features=categorical_features,
            generate_synthetic_data=generate_synthetic_data,
        )
        logger.info(f"Class object {self.__class__.__name__} instantiated successfully")


class FeatureImportanceRegression(FeatureImportanceBaseClass):
    def __init__(
        self,
        estimators: List[
            tuple[str, Union[BaseEstimator, ClassifierMixin, RegressorMixin]]
        ],
        data: pd.DataFrame,
        target_column: str,
        feature_set: list,
        numeric_features: list,
        categorical_features: list,
        generate_synthetic_data: bool = False,
        plot_importance: bool = False,
        test_size: float = 0.33,
    ):
        super().__init__(
            objective="regression",
            estimators=estimators,
            data=data,
            target_column=target_column,
            plot_importance=plot_importance,
            test_size=test_size,
            feature_set=feature_set,
            numeric_features=numeric_features,
            categorical_features=categorical_features,
            generate_synthetic_data=generate_synthetic_data,
        )
        logger.info(f"Class object {self.__class__.__name__} instantiated successfully")
